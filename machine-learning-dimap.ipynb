{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIM0782 - Machine Learning (DIMAp/UFRN/2024.1)\n",
    "\n",
    "## Preprocessing the data\n",
    "\n",
    "### Transforming non-structured data to structured data\n",
    "\n",
    "This is textual data, so the first step is to turn it into structured data by applying a transformer. I'm going to work mostly with BERT here.\n",
    "\n",
    "First, I'm going to define the imports block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reading the sentiments dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_df = pd.read_csv(\"datasets/twitter_sentiment_base_original.csv\", usecols=[\"text\", \"label\"])\n",
    "sentiments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating a Shorter Dataset (Optional Step)\n",
    "-- -------------------------\n",
    "\n",
    "This might be helpful in case your base is originally too big to be processed on a reasonable time for this exercise. Running this block would still allow you to run the subsequent blocks of code with no issues. You can totally skip this step as well in case you have the computing power to process the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size, random_state = 4000, 42\n",
    "sampled_data = []\n",
    "\n",
    "for i in range(6): ## Since we have 6 sentimens, labeled from 0 to 5\n",
    "    sampled_data.append(sentiments_df[sentiments_df['label'] == i].sample(n=sample_size, random_state=random_state))\n",
    "\n",
    "sentiments_df = pd.concat(sampled_data)\n",
    "sentiments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- ----------\n",
    "### End of Optional Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to utilize a BERT tokenizer to transform the text data that I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the pretrained model and tokenizer\n",
    "tokenizer = (ppb.DistilBertTokenizer).from_pretrained('distilbert-base-uncased')\n",
    "model = (ppb.DistilBertModel).from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "## Tokenizing w/ BERT\n",
    "sentiments_tokenized = sentiments_df[\"text\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above outputs the tokenized data with rows of different sizes, since each sentence has a different length. We now need to apply a process called padding in order to make all of the sentences with equal size. We will also need an attention mask, which will tell BERT which rows he should consider the data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating a new dataframe with the tokenized data appended with their associated labels\n",
    "biggest_sentence_length = 0\n",
    "for i in sentiments_tokenized.values:\n",
    "    if len(i) > biggest_sentence_length:\n",
    "        biggest_sentence_length = len(i)\n",
    "\n",
    "## Getting the values padded and the attention mask\n",
    "sentiments_tokenized_padded = np.array([i + [0]*(biggest_sentence_length-len(i)) for i in sentiments_tokenized.values])\n",
    "attention_mask = np.where(sentiments_tokenized_padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're finally going to process those input IDs that we generated on the steps above and we will generate the actual BERT embeddings. Given the size of the dataset, I ran multiple tests and realized that memory would be an issue. I need to split this into batches and work from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(sentiments_tokenized_padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "batch_size = 4000      # Change it as it is necessary \n",
    "all_hidden_states = [] # BERT generates what we call hidden states\n",
    "\n",
    "def batch_generator(input_ids, attention_mask, batch_size):\n",
    "    for i in range(0, len(input_ids), batch_size):\n",
    "        yield input_ids[i:i+batch_size], attention_mask[i:i+batch_size]\n",
    "\n",
    "## Processes the input over batches through the help of a generator\n",
    "with torch.no_grad():\n",
    "    for batch_ids, batch_mask in batch_generator(input_ids, attention_mask, batch_size):\n",
    "        outputs = model(batch_ids, attention_mask=batch_mask)\n",
    "        all_hidden_states.append(outputs.last_hidden_state)\n",
    "\n",
    "## Concatenates all batch results\n",
    "last_hidden_states = torch.cat(all_hidden_states, dim=0)\n",
    "torch.save(last_hidden_states, 'tensor.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = torch.load('datasets/tensor.pt')\n",
    "sentiments_features = last_hidden_states[:,0,:].numpy()\n",
    "labels = sentiments_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also going to develop this auxiliary block that will be able to save my dataset with the n-dimension features I generate as a result from applying the techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_csv(features, labels, filename):\n",
    "    df = pd.DataFrame(features, columns=[f'feature_{i}' for i in range(features.shape[1])])\n",
    "    df['label'] = labels.to_numpy()\n",
    "    \n",
    "    df.to_csv(filename, index=False)    \n",
    "\n",
    "## This is the line that will change depending on the file I want to generate\n",
    "features_to_csv(sentiments_features, labels, 'datasets/twitter_sentiment_base_ready.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction of Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this work, it is required to reduce instances on my dataset.\n",
    "\n",
    "The first dataset I was working with had about 400000 records, and I could not process the embeddings and tokenization on a timely manner. I had to do a brusk reduction on the size, so now I don't have a dataset that is that big. My goal here is just to get some sampling of 80% of the already reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables for the sampling conditions\n",
    "sample_size, random_state = 3200, 42\n",
    "sampled_data = []\n",
    "sentiments_processed_df = pd.read_csv('datasets/twitter_sentiment_base_ready.csv')\n",
    "\n",
    "for i in range(6): ## Since we have 6 sentimens, labeled from 0 to 5\n",
    "    sampled_data.append(sentiments_processed_df[sentiments_processed_df['label'] == i].sample(n=sample_size, random_state=random_state))\n",
    "\n",
    "sampled_sentiments_df = pd.concat(sampled_data)\n",
    "sampled_sentiments_df.to_csv('datasets/twitter_sentiments_base_sampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute selection (w/ Decision Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are quite useful for the purpose of doing attribute (or feature) selection because they inherently perform feature selection by splitting nodes based on the most informative features. By using a decision tree, we can identify which attributes (or features) contribute most to predicting the output, making it a practical approach for reducing dimensionality and improving model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentiments_embeddings_train, sentiments_embeddings_test, sentiments_labels_train, sentiments_labels_test = train_test_split(sentiments_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the classifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "decision_tree.fit(sentiments_embeddings_train, sentiments_labels_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = decision_tree.feature_importances_\n",
    "\n",
    "# Now we sort the importances on descending order and select the top X features - (X: originally 50)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "top_features = indices[:50]\n",
    "\n",
    "# Finally, we select only the top features that were applied\n",
    "sentiments_embeddings_train_reduced = sentiments_embeddings_train[:, top_features]\n",
    "sentiments_embeddings_test_reduced = sentiments_embeddings_test[:, top_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm adding this auxiliary block here for merging the results back to a single sentiments embeddings array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_embeddings_decision_tree = np.concatenate((sentiments_embeddings_train_reduced, sentiments_embeddings_test_reduced))\n",
    "sentiments_embeddings_decision_tree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Attribute selection using Decision Tree\")\n",
    "plt.bar(range(sentiments_embeddings_train_reduced.shape[1]), importances[top_features], align='center')\n",
    "plt.xticks(range(sentiments_embeddings_train_reduced.shape[1]), top_features, rotation=90)\n",
    "plt.xlim([-1, sentiments_embeddings_train_reduced.shape[1]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Running Principal Component Analysis (PCA) is a powerful method to reduce the dimensionality of our data while retaining as much variability as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "sentiments_features_scaled = scaler.fit_transform(sentiments_features)\n",
    "\n",
    "# Choose the number of components (example: reduce dimensions to keep 95% of the variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "sentiments_pca = pca.fit_transform(sentiments_features_scaled)\n",
    "\n",
    "print(f\"Number of components kept: {pca.n_components_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the explained variance of the PCA. This will tell us how much information (variability) was retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance by each component: {explained_variance}\")\n",
    "print(f\"Total variance explained: {sum(explained_variance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now finally visualize the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, 215), pca.explained_variance_ratio_, marker='o', linestyle='--')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scree plot generated is a graphical representation of the variance explained by each principal component the analysis.\n",
    "\n",
    "The x-axis represents the number of principal components. In the plot, it ranges from 1 to over 200.\n",
    "The y-axis represents the variance explained by each principal component. It shows the proportion of the dataset’s total variance.\n",
    "Elbow Method: Typically, the goal of a scree plot is to identify the 'elbow' of the graph, which indicates the point at which the variance explained by each additional component drops off and becomes minimal. This point is considered a good cut-off for reducing the number of components because beyond this point, you're getting diminishing returns on explained variance.\n",
    "\n",
    "In our plot, there is a steep drop after the first few components, then the decline slows significantly. This suggests that the first few components capture a substantial amount of the information (variance) in the dataset. After this initial steep drop, the plot levels off around the 40-component mark, indicating that each additional component contributes less and less.\n",
    "\n",
    "It’s worth noting that the first component explains significantly more variance than the subsequent ones. In practical terms, this could mean that there is one dominant feature or pattern in your data that accounts for most of the variance, with each additional feature contributing less to explaining the dataset.\n",
    "\n",
    "Based on this plot, we might consider retaining only the components before the curve starts to flatten if you aim to reduce dimensionality while retaining most of the information.\n",
    "\n",
    "That's why I'm going to run the PCA algorithm again, but changing the n parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_round_two = PCA(n_components=0.70)\n",
    "sentiments_pca = pca_round_two.fit_transform(sentiments_features_scaled)\n",
    "\n",
    "print(f\"Number of components kept: {pca_round_two.n_components_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate this scree plot again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, 30), pca_round_two.explained_variance_ratio_, marker='o', linestyle='--')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression\n",
    "\n",
    "Given that the dataset has 6 possible sentiments (labels), it won't be possible to run Logistic Regression (because the data decision is not binary). Because of that, we can investigate the behavior of applying a multinomial logistic regression classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Percentage test split method - can adjust percentage\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Invoking the logistic regression model\n",
    "logistic_regression_sentiments = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1500)\n",
    "\n",
    "logistic_regression_sentiments.fit(train_features, train_labels)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logistic_regression_sentiments.predict(test_features)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement the k-Nearest Neighbors algorithm and understand how it behaves with our sentiments dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Percentage test split method - can adjust percentage\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the KNN model with a specified number of neighbors; e.g., k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's actually run the kNN Algorithm, with the parameters adjusted as above. This version comprehends the percentage test split, but I'll also show it using the 10-fold cross validation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "knn.fit(train_features, train_labels)\n",
    "\n",
    "# Predicting the test set\n",
    "labels_predict = knn.predict(test_features)\n",
    "\n",
    "print(\"Accuracy of kNN (5 neighbors, 70/30 split):\", accuracy_score(test_labels, labels_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN via the 10-fold cross-validation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Create the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Prepare to collect accuracy scores\n",
    "accuracies = []\n",
    "\n",
    "knn_labels = np.array(labels)\n",
    "\n",
    "# Perform 10-fold CV\n",
    "for train_index, test_index in kf.split(sentiments_features):\n",
    "    sentiments_train, sentiments_test = sentiments_features[train_index], sentiments_features[test_index]\n",
    "    labels_train, labels_test = knn_labels[train_index], knn_labels[test_index]\n",
    "\n",
    "    # Train the kNN Model inside the current fold\n",
    "    knn.fit(sentiments_train, labels_train)\n",
    "\n",
    "    # Predict and evaluate the model\n",
    "    labels_predict = knn.predict(sentiments_test)\n",
    "    accuracy = accuracy_score(labels_test, labels_predict)\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"Accuracy from this fold:\", accuracy)\n",
    "\n",
    "# Calculate and print the average accuracy and standard deviation\n",
    "print(\"Average accuracy:\", np.mean(accuracies))\n",
    "print(\"Standard deviation of accuracy:\", np.std(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "Implementing the Decision Tree Classifier and varying its max depth to compare accuracy. This is using the percentage split method of collecting the data from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Percentage test split method - can adjust percentage\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the Decision Tree Classifier (DTC) model with a specified max depth; e.g., md =5\n",
    "dtc = DecisionTreeClassifier(max_depth=7, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "dtc.fit(train_features, train_labels)\n",
    "\n",
    "# Predicting the test set\n",
    "labels_predict = dtc.predict(test_features)\n",
    "\n",
    "print(\"Accuracy of Decision Tree Classifier (5 Max-Depth, 70/30 split):\", accuracy_score(test_labels, labels_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implementing it via the 10-fold cross-validation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Create the Decision Tree Classifier (DTC) model with a specified max depth; e.g., md =5\n",
    "dtc = DecisionTreeClassifier(max_depth=7, random_state=42)\n",
    "\n",
    "# Prepare to collect accuracy scores\n",
    "accuracies = []\n",
    "\n",
    "dtc_labels = np.array(labels)\n",
    "\n",
    "# Perform 10-fold CV\n",
    "for train_index, test_index in kf.split(sentiments_features):\n",
    "    sentiments_train, sentiments_test = sentiments_features[train_index], sentiments_features[test_index]\n",
    "    labels_train, labels_test = dtc_labels[train_index], dtc_labels[test_index]\n",
    "\n",
    "    # Train the DTC Model inside the current fold\n",
    "    dtc.fit(sentiments_train, labels_train)\n",
    "\n",
    "    # Predict and evaluate the model\n",
    "    labels_predict = dtc.predict(sentiments_test)\n",
    "    accuracy = accuracy_score(labels_test, labels_predict)\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"Accuracy from this fold:\", accuracy)\n",
    "\n",
    "# Calculate and print the average accuracy and standard deviation\n",
    "print(\"Average accuracy:\", np.mean(accuracies))\n",
    "print(\"Standard deviation of accuracy:\", np.std(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "Implementing the Gaussian Naive Bayes classifier. Parameters are priors = 0 (array of prior probabilities) and var_smoothing: 1e-9, as per the professor's specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Percentage test split method - can adjust percentage\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the Gaussian Naive Bayes (GNB) model\n",
    "gnb = GaussianNB(priors=None, var_smoothing=1e-9)\n",
    "\n",
    "# Training the model\n",
    "gnb.fit(train_features, train_labels)\n",
    "\n",
    "# Predicting the test set\n",
    "labels_predict = gnb.predict(test_features)\n",
    "\n",
    "print(\"Accuracy of Gaussian Naive Bayes:\", accuracy_score(test_labels, labels_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the 10-fold cross-validation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Create the Decision Tree Classifier (DTC) model with a specified max depth; e.g., md =5\n",
    "gnb = GaussianNB(priors=None, var_smoothing=1e-9)\n",
    "\n",
    "# Prepare to collect accuracy scores\n",
    "accuracies = []\n",
    "\n",
    "gnb_labels = np.array(labels)\n",
    "\n",
    "# Perform 10-fold CV\n",
    "for train_index, test_index in kf.split(sentiments_features):\n",
    "    sentiments_train, sentiments_test = sentiments_features[train_index], sentiments_features[test_index]\n",
    "    labels_train, labels_test = gnb_labels[train_index], gnb_labels[test_index]\n",
    "\n",
    "    # Train the GNB Model inside the current fold\n",
    "    gnb.fit(sentiments_train, labels_train)\n",
    "\n",
    "    # Predict and evaluate the model\n",
    "    labels_predict = gnb.predict(sentiments_test)\n",
    "    accuracy = accuracy_score(labels_test, labels_predict)\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"Accuracy from this fold:\", accuracy)\n",
    "\n",
    "# Calculate and print the average accuracy and standard deviation\n",
    "print(\"Average accuracy:\", np.mean(accuracies))\n",
    "print(\"Standard deviation of accuracy:\", np.std(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "The Multilayer Perceptron (MLP) is a type of artificial neural network that is widely used for solving complex pattern recognition and classification problems. It belongs to a larger class of feedforward neural networks and consists of at least three layers of nodes: an input layer, one or more hidden layers, and an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Percentage test split method - can adjust percentage\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Calculate the neurons configuration, based on requirements provided by the professor\n",
    "a_value = 387 # 768 attributes and 6 classes (sentiments)\n",
    "neurons = [a_value - 50, a_value, a_value + 50]\n",
    "accuracies = []\n",
    "\n",
    "# Loop through neuron configurations\n",
    "for neuron in neurons:\n",
    "    print(\"Starting the classifier for neurons amount:\", neuron)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(neuron,), max_iter=300, activation='relu', solver='adam', random_state=42)\n",
    "    mlp.fit(train_features, train_labels)\n",
    "    labels_predict = mlp.predict(test_features)\n",
    "    accuracy = accuracy_score(test_labels, labels_predict)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(neurons, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.title('MLP Accuracy vs. Number of Neurons')\n",
    "plt.xlabel('Number of Neurons')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to take the number of neurons that we found on the previous experiment, and run the classifier again, but varying the max iterations parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Percentage test split method - can adjust percentage\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initiate the values needed\n",
    "accuracies = []\n",
    "iterations = [300, 1000, 5000]\n",
    "best_neuron_length = 437\n",
    "\n",
    "# Loop through neuron configurations\n",
    "for iteration in iterations:\n",
    "    print(\"Starting the classifier for neurons iterations amount:\", iteration)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(best_neuron_length,), max_iter=iteration, activation='relu', solver='adam', random_state=42)\n",
    "    mlp.fit(train_features, train_labels)\n",
    "    labels_predict = mlp.predict(test_features)\n",
    "    accuracy = accuracy_score(test_labels, labels_predict)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.title('MLP Accuracy vs. Number of Iterations')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with all of that information, we're going to adjust the learning rate parameter changes, to look for the best configuration available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Percentage test split method - can adjust percentage\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initiate the values needed\n",
    "accuracies = []\n",
    "best_max_iter = 300\n",
    "best_neuron_length = 437\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Loop through neuron configurations\n",
    "for learning_rate in learning_rates:\n",
    "    print(\"Starting the classifier for learning rate amount:\", learning_rate)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(best_neuron_length,), learning_rate_init=learning_rate, max_iter=best_max_iter, activation='relu', solver='adam', random_state=42)\n",
    "    mlp.fit(train_features, train_labels)\n",
    "    labels_predict = mlp.predict(test_features)\n",
    "    accuracy = accuracy_score(test_labels, labels_predict)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(learning_rates, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.title('MLP Accuracy vs. Learning Rate')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate the neurons configuration, based on requirements provided by the professor\n",
    "a_value = 387 # 768 attributes and 6 classes (sentiments)\n",
    "neurons = [a_value - 50, a_value, a_value + 50]\n",
    "accuracies = []\n",
    "\n",
    "# Perform 10-fold CV for each neuron that I'm evaluating\n",
    "for neuron in neurons:\n",
    "    print(\"Entering neuron amount of:\", neuron)\n",
    "    local_accuracy_average = []\n",
    "    for train_index, test_index in kf.split(sentiments_features):\n",
    "        sentiments_train, sentiments_test = sentiments_features[train_index], sentiments_features[test_index]\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(neuron,), max_iter=300, activation='relu', solver='adam', random_state=42)\n",
    "        mlp.fit(sentiments_train, labels_train)\n",
    "        labels_predict = mlp.predict(sentiments_test)\n",
    "        local_accuracy_average.append(accuracy_score(labels_test, labels_predict))\n",
    "    print(\"Processed neurons with 10-fold cross-validation, average accuracy:\", np.mean(local_accuracy_average))\n",
    "    accuracies.append(np.mean(local_accuracy_average))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(neurons, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.title('MLP Accuracy vs. Number of Neurons (10-fold CV)')\n",
    "plt.xlabel('Number of Neurons')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with all of that information, we're going to adjust the learning rate parameter changes, to look for the best configuration available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate the neurons configuration, based on requirements provided by the professor\n",
    "iterations = [300, 1000, 5000]\n",
    "best_neuron_length = 387\n",
    "accuracies = []\n",
    "\n",
    "# Perform 10-fold CV for each neuron that I'm evaluating\n",
    "for iteration in iterations:\n",
    "    print(\"Entering iteration amount of:\", iteration)\n",
    "    local_accuracy_average = []\n",
    "    for train_index, test_index in kf.split(sentiments_features):\n",
    "        sentiments_train, sentiments_test = sentiments_features[train_index], sentiments_features[test_index]\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(best_neuron_length,), max_iter=iteration, activation='relu', solver='adam', random_state=42)\n",
    "        mlp.fit(sentiments_train, labels_train)\n",
    "        labels_predict = mlp.predict(sentiments_test)\n",
    "        local_accuracy_average.append(accuracy_score(labels_test, labels_predict))\n",
    "    print(\"Processed iteration with 10-fold cross-validation, average accuracy:\", np.mean(local_accuracy_average))\n",
    "    accuracies.append(np.mean(local_accuracy_average))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.title('MLP Accuracy vs. Number of Iterations (10-fold CV)')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final running stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate the neurons configuration, based on requirements provided by the professor\n",
    "best_max_iter = 300\n",
    "best_neuron_length = 387\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "accuracies = []\n",
    "\n",
    "# Perform 10-fold CV for each neuron that I'm evaluating\n",
    "for learning_rate in learning_rates:\n",
    "    print(\"Entering learning rate amount of:\", learning_rate)\n",
    "    local_accuracy_average = []\n",
    "    for train_index, test_index in kf.split(sentiments_features):\n",
    "        sentiments_train, sentiments_test = sentiments_features[train_index], sentiments_features[test_index]\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(best_neuron_length,), max_iter=best_max_iter, learning_rate_init=learning_rate, activation='relu', solver='adam', random_state=42)\n",
    "        mlp.fit(sentiments_train, labels_train)\n",
    "        labels_predict = mlp.predict(sentiments_test)\n",
    "        local_accuracy_average.append(accuracy_score(labels_test, labels_predict))\n",
    "    print(\"Processed learning rate with 10-fold cross-validation, average accuracy:\", np.mean(local_accuracy_average))\n",
    "    accuracies.append(np.mean(local_accuracy_average))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(learning_rates, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.title('MLP Accuracy vs. Learning Rate (10-fold CV)')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for parameters using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Percentage test split method - can adjust percentage\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# The parameters we will pass to GridSearch\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(450,),(387,),(400,)],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'random_state': [1,50],\n",
    "    'max_iter': [300]\n",
    "}\n",
    "\n",
    "# Create MLPClassifier object\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# Setting up the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=3,verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fitting GridSearchCV\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Best parameters found by GridSearchCV\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model found from the grid search\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_pred = best_mlp.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "\n",
    "print(\"Accuracy of the best model: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP With the Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "datasets_to_explore = ['datasets/twitter_sentiment_base_ready.csv', 'datasets/twitter_sentiments_base_sampled.csv', 'datasets/twitter_sentiment_base_decision_tree.csv', 'datasets/twitter_sentiment_base_pca.csv']\n",
    "\n",
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#best_max_iter = 300\n",
    "#best_neuron_length = 387\n",
    "#best_learning_rate = 0.001\n",
    "\n",
    "best_max_iter = 300\n",
    "best_neuron_length = 387\n",
    "best_learning_rate = 0.01\n",
    "\n",
    "for dataset in datasets_to_explore:\n",
    "    print('Entering dataset ' + dataset)\n",
    "    working_base = pd.read_csv(dataset)\n",
    "    sentiments_features = working_base.loc[:, working_base.columns.str.startswith('feature_')].to_numpy()\n",
    "    labels = working_base['label'].to_numpy()\n",
    "    accuracies = []\n",
    "\n",
    "    # Perform 10-fold CV\n",
    "    for train_index, test_index in kf.split(sentiments_features):\n",
    "        sentiments_train, sentiments_test = sentiments_features[train_index], sentiments_features[test_index]\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(best_neuron_length,), max_iter=best_max_iter, learning_rate_init=best_learning_rate, activation='relu', solver='sgd', random_state=42)\n",
    "        mlp.fit(sentiments_train, labels_train)\n",
    "        labels_predict = mlp.predict(sentiments_test)\n",
    "        accuracies.append(accuracy_score(labels_test, labels_predict))\n",
    "    \n",
    "    content = 'Dataset: ' + dataset + ', 10-Fold CV Average Accuracy: ' + str(np.mean(accuracies))\n",
    "    print(content)\n",
    "    with open('datasets/results_grid.txt', \"a\") as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "We're now going to evaluate how a few unsupervised machine learning algorithms work, gettings started with the k-Means. We will vary the amount of iterations to try to find the best parameters for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "db_scores = []\n",
    "silhouette_scores = []\n",
    "k_values = range(2, 21)\n",
    "\n",
    "for k in k_values:\n",
    "    # Helper variables for calculating the average score after running the algorithm 5 times\n",
    "    current_db_scores = []\n",
    "    current_silhouette_scores = []\n",
    "    for seed in range(5):\n",
    "        print(f\"Entering k={k} iteration of the K-Means, iteration number { seed }.\")\n",
    "        # Runs K-Means 5 times for the current K iteration\n",
    "        kmeans = KMeans(n_clusters=k, random_state=seed, n_init=1)\n",
    "        labels = kmeans.fit_predict(sentiments_features)\n",
    "        # Calculate the scores of interest\n",
    "        db_score = davies_bouldin_score(sentiments_features, labels)\n",
    "        silhouette_grade = silhouette_score(sentiments_features, labels)\n",
    "        current_db_scores.append(db_score)\n",
    "        current_silhouette_scores.append(silhouette_grade)\n",
    "    # Calculate the scores for the current K\n",
    "    db_scores.append(np.mean(current_db_scores))\n",
    "    silhouette_scores.append(np.mean(current_silhouette_scores))\n",
    "    print(f\"Average Davies-Bouldin Score for k={k}: {np.mean(current_db_scores)}\")\n",
    "    print(f\"Average Silhouette Score for k={k}: {np.mean(current_silhouette_scores)}\")\n",
    "    with open('datasets/results_k_means.txt', \"a\") as file:\n",
    "        file.write(f\"Average Davies-Bouldin Score for k={k}: {np.mean(current_db_scores)}\")\n",
    "        file.write(f\"Average Silhouette Score for k={k}: {np.mean(current_silhouette_scores)}\")\n",
    "\n",
    "# Plotting results\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Average Davies-Bouldin Score', color=color)\n",
    "ax1.plot(k_values, db_scores, marker='o', linestyle='-', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "'''ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Average Silhouette Score', color=color)\n",
    "ax2.plot(k_values, silhouette_scores, marker='o', linestyle='-', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)'''\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.title('Clustering Performance Evaluation')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Implementing this unsupervised machine learning method and then varying the parameters to find the best configuration available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Range of k values to test\n",
    "k_values = range(2, 21)\n",
    "db_scores = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Agglomerative Clustering without specifying the number of clusters\n",
    "clustering = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage='ward')\n",
    "clustering.fit(sentiments_features)\n",
    "\n",
    "# Extracting the number of clusters and their labels at different cuts of the dendrogram\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Generate the linkage matrix\n",
    "Z = linkage(sentiments_features, method='ward')\n",
    "\n",
    "for k in k_values:\n",
    "    # Cutting the dendrogram at the specified number of clusters\n",
    "    labels = fcluster(Z, k, criterion='maxclust')\n",
    "    \n",
    "    # Calculate and store the Davies-Bouldin score\n",
    "    db_score = davies_bouldin_score(sentiments_features, labels)\n",
    "    db_scores.append(db_score)\n",
    "    \n",
    "    # Calculate and store the Silhouette score\n",
    "    silhouette = silhouette_score(sentiments_features, labels)\n",
    "    silhouette_scores.append(silhouette)\n",
    "    \n",
    "    print(f\"Number of clusters: {k}\")\n",
    "    print(f\"Davies-Bouldin Score: {db_score}\")\n",
    "    print(f\"Silhouette Score: {silhouette}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, db_scores, marker='o', linestyle='-', color='red')\n",
    "plt.title('Davies-Bouldin Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Davies-Bouldin Score')\n",
    "plt.grid(True)\n",
    "\n",
    "'''\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, silhouette_scores, marker='o', linestyle='-', color='blue')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)'''\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why distance_threshold=0 and n_clusters=None?\n",
    "\n",
    "By setting n_clusters=None and distance_threshold=0, we instruct AgglomerativeClustering to not stop merging clusters at a particular number of clusters or a specific distance threshold. Instead, it continues merging clusters until all points are merged into a single cluster, building the complete hierarchy or dendrogram. This full dendrogram captures all possible mergings from each point being its own cluster to all points being in one cluster.\n",
    "\n",
    "Once the full dendrogram is constructed, we can efficiently \"cut\" the dendrogram at different levels (different values of k) to explore the structure of the data without having to re-run the clustering algorithm each time. This is much more efficient than re-instantiating and re-running the AgglomerativeClustering for each k, especially for large datasets.\n",
    "\n",
    "If we were to instantiate AgglomerativeClustering within the loop for each k, it would mean recalculating the same hierarchical relationships multiple times: one for each value of k. This recalculating is unnecessary because the hierarchical relationships between data points don't change as k changes; only the level at which we decide to \"cut\" the tree changes.\n",
    "\n",
    "Finally, by generating the full dendrogram once, we capture all possible clustering configurations. We can then efficiently explore different configurations by adjusting where you cut the dendrogram. This is particularly useful for exploratory data analysis, where we might not know in advance what the optimal number of clusters should be and want to examine various possibilities quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "# Cluster using K-Means\n",
    "kmeans = KMeans(n_clusters=2, random_state=42).fit(sentiments_features)\n",
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "# Cluster using Hierarchical Clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=2).fit(sentiments_features)\n",
    "hierarchical_labels = hierarchical.labels_\n",
    "\n",
    "# Calculate metrics for K-Means\n",
    "silhouette_kmeans = silhouette_score(sentiments_features, kmeans_labels)\n",
    "db_index_kmeans = davies_bouldin_score(sentiments_features, kmeans_labels)\n",
    "\n",
    "# Calculate metrics for Hierarchical Clustering\n",
    "silhouette_hierarchical = silhouette_score(sentiments_features, hierarchical_labels)\n",
    "db_index_hierarchical = davies_bouldin_score(sentiments_features, hierarchical_labels)\n",
    "\n",
    "# Output the results\n",
    "print(\"K-Means Silhouette Score:\", silhouette_kmeans)\n",
    "print(\"Hierarchical Silhouette Score:\", silhouette_hierarchical)\n",
    "print(\"K-Means Davies-Bouldin Index:\", db_index_kmeans)\n",
    "print(\"Hierarchical Davies-Bouldin Index:\", db_index_hierarchical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles: Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "test_splits = [0.3, 0.2, 0.1]\n",
    "n_values = [10, 20]\n",
    "estimators = [ MLPClassifier(max_iter=300, hidden_layer_sizes=(387,), activation='relu', solver='adam', random_state=42) ] # KNeighborsClassifier(), GaussianNB(), MLPClassifier(max_iter=300, hidden_layer_sizes=(387,))]\n",
    "max_features = [0.3, 0.5, 0.8]\n",
    "\n",
    "# Holdout strategy (10/90, 20/80, 30/70)\n",
    "for test_split in test_splits:\n",
    "    for estimator in estimators:\n",
    "        for max_feature in max_features:\n",
    "            for n in n_values:\n",
    "                train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=test_split, random_state=42)\n",
    "                bagging = BaggingClassifier(estimator=estimator, max_features=max_feature, n_estimators=n, random_state=42)\n",
    "                bagging.fit(train_features, train_labels)\n",
    "                labels_predict = bagging.predict(test_features)\n",
    "                accuracy = accuracy_score(test_labels, labels_predict)\n",
    "                display_message = f\"Accuracy of Bagging Classifier ({estimator} as the Base Estimator) with {n} estimators, { max_feature } max features and {test_split} test split: {accuracy}\"\n",
    "                print(display_message)\n",
    "                with open('datasets/bagging_accuracies.txt', \"a\") as file:\n",
    "                    file.write(display_message + \"\\n\")\n",
    "\n",
    "# Cross-validation strategy\n",
    "for estimator in estimators:\n",
    "    for max_feature in max_features:\n",
    "        for n in n_values:\n",
    "            bagging = BaggingClassifier(estimator=estimator, max_features=max_feature, n_estimators=n, random_state=42)\n",
    "            scores = cross_val_score(bagging, sentiments_features, labels, cv=10, scoring='accuracy')\n",
    "            mean_accuracy = np.mean(scores)\n",
    "            display_message = f\"10-fold CV Accuracy of Bagging Classifier ({type(estimator).__name__} as the Base Estimator) with {n} estimators and { max_feature } max features: {mean_accuracy}\"\n",
    "            print(display_message)\n",
    "            with open('datasets/bagging_accuracies.txt', \"a\") as file:\n",
    "                file.write(display_message + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles: Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Wrapper for KNeighborsClassifier to ignore sample_weight\n",
    "class KNeighborsClassifierWrapper(KNeighborsClassifier):\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        return super().fit(X, y)\n",
    "\n",
    "# Wrapper for GaussianNB to ignore sample_weight\n",
    "class GaussianNBWrapper(GaussianNB):\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        return super().fit(X, y)\n",
    "\n",
    "# Wrapper for MLPClassifier to ignore sample_weight\n",
    "class MLPClassifierWrapper(MLPClassifier):\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        return super().fit(X, y)\n",
    "\n",
    "test_splits = [0.3, 0.2, 0.1]\n",
    "n_estimators = [10, 20]\n",
    "estimators = [\n",
    "    DecisionTreeClassifier(),\n",
    "    KNeighborsClassifierWrapper(),\n",
    "    GaussianNBWrapper(),\n",
    "    MLPClassifierWrapper(max_iter=300, hidden_layer_sizes=(387,))\n",
    "]\n",
    "\n",
    "# Assuming sentiments_features and labels are defined and preprocessed\n",
    "# Holdout strategy (10/90, 20/80, 30/70)\n",
    "for test_split in test_splits:\n",
    "    for estimator in estimators:\n",
    "        for n in n_estimators:\n",
    "            train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=test_split, random_state=42)\n",
    "            adaBooster = AdaBoostClassifier(estimator=estimator, n_estimators=n, algorithm=\"SAMME\", random_state=42)\n",
    "            adaBooster.fit(train_features, train_labels)\n",
    "            labels_predict = adaBooster.predict(test_features)\n",
    "            accuracy = accuracy_score(test_labels, labels_predict)\n",
    "            display_message = f\"Accuracy of ADA Boosting ({estimator.__class__.__name__} as the Base Estimator) with {n} estimators and {test_split} test split: {accuracy}\"\n",
    "            print(display_message)\n",
    "            with open('datasets/boosting_accuracies.txt', \"a\") as file:\n",
    "                file.write(display_message + \"\\n\")\n",
    "\n",
    "# Cross-validation strategy\n",
    "for estimator in estimators:\n",
    "    for n in n_estimators:\n",
    "        adaBooster = AdaBoostClassifier(estimator=estimator, n_estimators=n, algorithm=\"SAMME\", random_state=42)\n",
    "        scores = cross_val_score(adaBooster, sentiments_features, labels, cv=10, scoring='accuracy')\n",
    "        mean_accuracy = np.mean(scores)\n",
    "        display_message = f\"10-fold CV Accuracy of ADA Boosting ({type(estimator).__name__} as the Base Estimator) with {n} estimators: {mean_accuracy}\"\n",
    "        print(display_message)\n",
    "        with open('datasets/boosting_accuracies.txt', \"a\") as file:\n",
    "            file.write(display_message + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "test_splits = [0.3, 0.2, 0.1]\n",
    "criterions = ['gini', 'entropy', 'log_loss']\n",
    "n_estimators = [10, 100]\n",
    "\n",
    "# Holdout strategy (10/90, 20/80, 30/70)\n",
    "for test_split in test_splits:\n",
    "    for criterion in criterions:\n",
    "        for n in n_estimators:\n",
    "            train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=test_split, random_state=42)\n",
    "            randomForest = RandomForestClassifier(criterion=criterion, n_estimators=n, random_state=42)\n",
    "            randomForest.fit(train_features, train_labels)\n",
    "            labels_predict = randomForest.predict(test_features)\n",
    "            accuracy = accuracy_score(test_labels, labels_predict)\n",
    "            display_message = f\"Accuracy of Random Forest Classifier with {n} estimators, { criterion } criterion and {test_split} test split: {accuracy}\"\n",
    "            print(display_message)\n",
    "            with open('datasets/random_forest_accuracies.txt', \"a\") as file:\n",
    "                file.write(display_message + \"\\n\")\n",
    "\n",
    "# Cross-validation strategy\n",
    "for criterion in criterions:\n",
    "    for n in n_estimators:\n",
    "        randomForest = RandomForestClassifier(criterion=criterion, n_estimators=n, random_state=42)\n",
    "        scores = cross_val_score(randomForest, sentiments_features, labels, cv=10, scoring='accuracy')\n",
    "        mean_accuracy = np.mean(scores)\n",
    "        display_message = f\"10-fold CV Accuracy of Random Forest Classifier with {n} estimators and { criterion } criterion: {mean_accuracy}\"\n",
    "        print(display_message)\n",
    "        with open('datasets/random_forest_accuracies.txt', \"a\") as file:\n",
    "            file.write(display_message + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "test_splits = [0.3, 0.2, 0.1]\n",
    "base_estimators_10 = [('MLP01', MLPClassifier(hidden_layer_sizes=(10), max_iter=1000)),\n",
    "                      ('MLP02', MLPClassifier(hidden_layer_sizes=(8), max_iter=1000)),\n",
    "                      ('MLP03', MLPClassifier(hidden_layer_sizes=(6), max_iter=1000)),\n",
    "                      ('MLP04', MLPClassifier(hidden_layer_sizes=(4), max_iter=1000)),\n",
    "                      ('MLP05', MLPClassifier(hidden_layer_sizes=(2), max_iter=1000)),\n",
    "                      ('kNN01', KNeighborsClassifier(n_neighbors=1)),\n",
    "                      ('kNN02', KNeighborsClassifier(n_neighbors=2)),\n",
    "                      ('kNN03', KNeighborsClassifier(n_neighbors=3)),\n",
    "                      ('kNN04', KNeighborsClassifier(n_neighbors=4)),\n",
    "                      ('kNN05', KNeighborsClassifier(n_neighbors=5))]\n",
    "base_estimators_20 = base_estimators_10 + [('MLP06', MLPClassifier(hidden_layer_sizes=(12), max_iter=1000)),\n",
    "                                           ('MLP07', MLPClassifier(hidden_layer_sizes=(14), max_iter=1000)),\n",
    "                                           ('MLP08', MLPClassifier(hidden_layer_sizes=(16), max_iter=1000)),\n",
    "                                           ('MLP09', MLPClassifier(hidden_layer_sizes=(18), max_iter=1000)),\n",
    "                                           ('MLP10', MLPClassifier(hidden_layer_sizes=(20), max_iter=1000)),\n",
    "                                           ('kNN06', KNeighborsClassifier(n_neighbors=6)),\n",
    "                                           ('kNN07', KNeighborsClassifier(n_neighbors=7)),\n",
    "                                           ('kNN08', KNeighborsClassifier(n_neighbors=8)),\n",
    "                                           ('kNN09', KNeighborsClassifier(n_neighbors=9)),\n",
    "                                           ('kNN10', KNeighborsClassifier(n_neighbors=10))]\n",
    "base_estimators = [base_estimators_10, base_estimators_20]   \n",
    "\n",
    "# Holdout strategy (10/90, 20/80, 30/70)\n",
    "for test_split in test_splits:\n",
    "    for base_estimator in base_estimators:\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(sentiments_features, labels, test_size=test_split, random_state=42)\n",
    "        stacking = StackingClassifier(estimators=base_estimator)\n",
    "        stacking.fit(train_features, train_labels)\n",
    "        labels_predict = stacking.predict(test_features)\n",
    "        accuracy = accuracy_score(test_labels, labels_predict)\n",
    "        display_message = f\"Accuracy of Stacking Classifier with { len(base_estimator) } base estimators and {test_split} test split: {accuracy}\"\n",
    "        print(display_message)\n",
    "        with open('datasets/stacking_accuracies.txt', \"a\") as file:\n",
    "            file.write(display_message + \"\\n\")\n",
    "\n",
    "# Cross-validation strategy\n",
    "for base_estimator in base_estimators:\n",
    "    stacking = StackingClassifier(estimators=base_estimator)\n",
    "    scores = cross_val_score(stacking, sentiments_features, labels, cv=10, scoring='accuracy')\n",
    "    mean_accuracy = np.mean(scores)\n",
    "    display_message = f\"10-fold CV Accuracy of Stacking Classifier with { len(base_estimator) } estimators: {mean_accuracy}\"\n",
    "    print(display_message)\n",
    "    with open('datasets/stacking_accuracies.txt', \"a\") as file:\n",
    "        file.write(display_message + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friedman Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test statistic: 9.899999999999991\n",
      "p-value: 0.019435580728320332\n",
      "Significant differences detected (p < 0.05)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "# Step 1: Organize the data\n",
    "# Rows: Datasets, Columns: Algorithms\n",
    "accuracy_data = np.array([\n",
    "    [35.03, 29.28, 39.50, 58.96],  # Original Dataset\n",
    "    [34.28, 29.71, 40.06, 58.26],  # Reduced Dataset 1\n",
    "    [16.15, 16.39, 16.72, 16.49],  # Reduced Dataset 2\n",
    "    [32.27, 28.61, 41.84, 38.97]   # Reduced Dataset 3\n",
    "])\n",
    "\n",
    "# Step 2: Perform the Friedman test\n",
    "stat, p = friedmanchisquare(*[accuracy_data[:, i] for i in range(accuracy_data.shape[1])])\n",
    "\n",
    "# Step 3: Print the results\n",
    "print('Friedman test statistic:', stat)\n",
    "print('p-value:', p)\n",
    "\n",
    "# Interpretation\n",
    "if p < 0.05:\n",
    "    print(\"Significant differences detected (p < 0.05)\")\n",
    "else:\n",
    "    print(\"No significant differences detected (p >= 0.05)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  1.000000  0.900000  0.065622  0.823993\n",
      "1  0.900000  1.000000  0.065622  0.823993\n",
      "2  0.065622  0.065622  1.000000  0.354855\n",
      "3  0.823993  0.823993  0.354855  1.000000\n"
     ]
    }
   ],
   "source": [
    "import scikit_posthocs as sp\n",
    "import numpy as np\n",
    "\n",
    "# Data as provided\n",
    "accuracy_data = np.array([\n",
    "    [35.03, 29.28, 39.50, 58.96],  # Original Dataset\n",
    "    [34.28, 29.71, 40.06, 58.26],  # Reduced Dataset 1\n",
    "    [16.15, 16.39, 16.72, 16.49],  # Reduced Dataset 2\n",
    "    [32.27, 28.61, 41.84, 38.97]   # Reduced Dataset 3\n",
    "])\n",
    "\n",
    "# Perform the Nemenyi post-hoc test\n",
    "nemenyi_results = sp.posthoc_nemenyi_friedman(accuracy_data.T)\n",
    "\n",
    "# Print results\n",
    "print(nemenyi_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon test statistic for DB Index: 0.0, p-value: 1.0\n",
      "Wilcoxon test statistic for Silhouette score: 0.0, p-value: 1.0\n",
      "No significant differences detected in DB Index (p >= 0.05)\n",
      "No significant differences detected in Silhouette score (p >= 0.05)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Step 1: Prepare your data\n",
    "db_index = [2.2472, 2.4941]  # Davies-Bouldin Index for k-Means and Hierarchical Clustering\n",
    "silhouette = [0.1746, 0.1194]  # Silhouette score for k-Means and Hierarchical Clustering\n",
    "\n",
    "# Step 2: Perform the Wilcoxon signed-rank test\n",
    "stat_db, p_db = wilcoxon([2.2472], [2.4941])\n",
    "stat_sil, p_sil = wilcoxon([0.1746], [0.1194])\n",
    "\n",
    "# Step 3: Print the results\n",
    "print(f'Wilcoxon test statistic for DB Index: {stat_db}, p-value: {p_db}')\n",
    "print(f'Wilcoxon test statistic for Silhouette score: {stat_sil}, p-value: {p_sil}')\n",
    "\n",
    "# Interpretation\n",
    "if p_db < 0.05:\n",
    "    print(\"Significant differences detected in DB Index (p < 0.05)\")\n",
    "else:\n",
    "    print(\"No significant differences detected in DB Index (p >= 0.05)\")\n",
    "\n",
    "if p_sil < 0.05:\n",
    "    print(\"Significant differences detected in Silhouette score (p < 0.05)\")\n",
    "else:\n",
    "    print(\"No significant differences detected in Silhouette score (p >= 0.05)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test statistic: 6.599999999999994\n",
      "p-value: 0.0858010874001231\n",
      "No significant differences detected (p >= 0.05)\n",
      "         0         1    2    3         4         5         6\n",
      "0  1.00000  0.900000  0.9  0.9  0.900000  0.829610  0.900000\n",
      "1  0.90000  1.000000  0.9  0.9  0.900000  0.900000  0.637136\n",
      "2  0.90000  0.900000  1.0  0.9  0.900000  0.900000  0.900000\n",
      "3  0.90000  0.900000  0.9  1.0  0.900000  0.900000  0.900000\n",
      "4  0.90000  0.900000  0.9  0.9  1.000000  0.900000  0.540899\n",
      "5  0.82961  0.900000  0.9  0.9  0.900000  1.000000  0.440184\n",
      "6  0.90000  0.637136  0.9  0.9  0.540899  0.440184  1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Data Matrix\n",
    "accuracy_data = np.array([\n",
    "    [36.17, 37.80125, 39.51875, 62.43875],  # Bagging Default\n",
    "    [35.37875, 39.17875, 39.27375, 60.6575],  # Bagging Max Features 0.3\n",
    "    [35.935, 38.52875, 39.2775, 61.30875],  # Bagging Max Features 0.5\n",
    "    [35.67375, 38.0925, 39.385, 61.72875],  # Bagging Max Features 0.8\n",
    "    [31.81375, 36.975, 39.5425, 61.19625],  # Boosting Default\n",
    "    [38.265, 37.895, 37.88, 37.54],  # Random Forest\n",
    "    [60.1, 60.705, 61.235, 59.885],  # Stacking\n",
    "])\n",
    "\n",
    "# Perform the Friedman test\n",
    "stat, p = friedmanchisquare(*[accuracy_data[:, i] for i in range(accuracy_data.shape[1])])\n",
    "\n",
    "print('Friedman test statistic:', stat)\n",
    "print('p-value:', p)\n",
    "\n",
    "# Interpretation\n",
    "if p < 0.05:\n",
    "    print(\"Significant differences detected (p < 0.05)\")\n",
    "else:\n",
    "    print(\"No significant differences detected (p >= 0.05)\")\n",
    "\n",
    "# Perform the Nemenyi post-hoc test\n",
    "nemenyi_results = sp.posthoc_nemenyi_friedman(accuracy_data.T)\n",
    "\n",
    "# Print results\n",
    "print(nemenyi_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of the Art for Sentiment Analysis: BERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will fine tune the BERT model with the sentiments dataset, in order to look out for a better accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Set device to CPU explicitly\n",
    "device = torch.device('cuda')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Read dataset using pandas\n",
    "sentiments_df = pd.read_csv(\"datasets/twitter_sentiment_base_original.csv\", usecols=[\"text\", \"label\"])\n",
    "\n",
    "# Get unique labels using pandas\n",
    "unique_labels = sorted(sentiments_df['label'].unique())\n",
    "class_label = ClassLabel(num_classes=len(unique_labels), names=list(map(str, unique_labels)))\n",
    "\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(sentiments_df)\n",
    "\n",
    "# Encode labels to ClassLabel\n",
    "dataset = dataset.cast_column(\"label\", class_label)\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_data(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_data = dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "# Rename the column from \"label\" to \"labels\" for compatibility with BertForSequenceClassification\n",
    "tokenized_data = tokenized_data.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_test_split = tokenized_data.train_test_split(test_size=0.3, stratify_by_column='labels')\n",
    "train_data = train_test_split['train']\n",
    "test_data = train_test_split['test']\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "# Load pre-trained BERT model and move to CPU\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
    "model.to('cuda')  # Move model to CPU\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500, \n",
    "    weight_decay=0.01, \n",
    "    logging_dir='./results/logs', \n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Use default data collator\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_data, \n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the fine tuning process is complete and we have access to their results, we can try spinning out something new now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-readable Predictions: ['joy', 'joy']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./fine-tuned-bert', num_labels=6)\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine-tuned-bert')\n",
    "\n",
    "# Move model to CPU if not already\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the label mapping\n",
    "label_mapping = {\n",
    "    0: \"sadness\",\n",
    "    1: \"joy\",\n",
    "    2: \"love\",\n",
    "    3: \"anger\",\n",
    "    4: \"fear\",\n",
    "    5: \"surprise\"\n",
    "}\n",
    "\n",
    "# Function to predict sentiment of new texts\n",
    "def predict(texts):\n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "    \n",
    "    # Perform inference with the model\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits (raw predictions) from the model outputs\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to predicted class labels\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Move predictions back to the CPU and convert to numpy array\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    # Map the numerical predictions to human-readable labels\n",
    "    human_readable_predictions = [label_mapping[pred] for pred in predictions]\n",
    "    \n",
    "    return predictions, human_readable_predictions\n",
    "\n",
    "# Example usage: Predict sentiment of new texts\n",
    "texts = [\"surprise!\", \"In a hole in the ground there once lived happy hobbit.\"]\n",
    "predictions, human_readable_predictions = predict(texts)\n",
    "print(\"Human-readable Predictions:\", human_readable_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate this with the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>I'm so in love with you. Every moment with you...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>You are my everything. I can't imagine life wi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>My heart skips a beat every time I see you.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>I love you more than words can express.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>You make my world a better place just by being...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>My love for you is eternal.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>You are my everything, my true love.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>My heart is yours, always.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>You are my everything, forever.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>My love for you is endless.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "201  I'm so in love with you. Every moment with you...      2\n",
       "202  You are my everything. I can't imagine life wi...      2\n",
       "203        My heart skips a beat every time I see you.      2\n",
       "204            I love you more than words can express.      2\n",
       "205  You make my world a better place just by being...      2\n",
       "..                                                 ...    ...\n",
       "294                        My love for you is eternal.      2\n",
       "295               You are my everything, my true love.      2\n",
       "296                         My heart is yours, always.      2\n",
       "297                    You are my everything, forever.      2\n",
       "298                        My love for you is endless.      2\n",
       "\n",
       "[98 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('datasets/test_sentiments.csv', usecols=['text', 'label'])\n",
    "test_data.query('label == 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 5, does not match size of target_names, 6. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m predicted_labels, human_readable_predictions \u001b[38;5;241m=\u001b[39m predict(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())  \u001b[38;5;66;03m# Convert text column to list\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Generate a classification report\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2648\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2642\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2643\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2644\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[1;32m   2645\u001b[0m             )\n\u001b[1;32m   2646\u001b[0m         )\n\u001b[1;32m   2647\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2648\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2650\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[1;32m   2652\u001b[0m         )\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2654\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 5, does not match size of target_names, 6. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv('datasets/test_sentiments.csv', usecols=['text', 'label'])\n",
    "test_data = test_data.query('label == 2')\n",
    "\n",
    "# Evaluating the model on the test dataset\n",
    "true_labels = test_data['label'].tolist()  # Convert to list\n",
    "predicted_labels, human_readable_predictions = predict(test_data['text'].tolist())  # Convert text column to list\n",
    "\n",
    "# Generate a classification report\n",
    "print(classification_report(true_labels, predicted_labels, target_names=list(label_mapping.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Fine Tuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sadness: Precision (0.94), Recall (0.99), F1-Score (0.96)\n",
    "The model is very good at correctly predicting sadness (high recall), and most of its sadness predictions are correct (high precision).\n",
    "\n",
    "#### Joy: Precision (0.99), Recall (0.91), F1-Score (0.95)\n",
    "The model is excellent at predicting joy (high precision), but it misses some joy instances (slightly lower recall).\n",
    "\n",
    "#### Love: Precision (0.92), Recall (1.00), F1-Score (0.96)\n",
    "The model identifies all instances of love perfectly (recall of 1.00), though a small number of love predictions might be incorrect.\n",
    "\n",
    "#### Anger: Precision (0.93), Recall (0.99), F1-Score (0.96)\n",
    "The model is very good at both identifying anger and correctly predicting it.\n",
    "\n",
    "#### Fear: Precision (0.78), Recall (0.86), F1-Score (0.82)\n",
    "The model's performance for fear is lower compared to other emotions, indicating it misses some fear instances and also has more incorrect fear predictions.\n",
    "\n",
    "#### Surprise: Precision (0.99), Recall (0.75), F1-Score (0.86)\n",
    "The model is excellent at correctly predicting surprise when it does so (high precision), but it misses a significant number of surprise instances (lower recall).\n",
    "\n",
    "#### Overall Performance:\n",
    "\n",
    "#### Accuracy: 0.92\n",
    "The model correctly classifies 92% of all instances, which is very good.\n",
    "\n",
    "#### Macro Average: Precision: 0.92, Recall: 0.92, F1-Score: 0.92\n",
    "This indicates a balanced performance across all classes.\n",
    "\n",
    "#### Weighted Average: Precision: 0.92, Recall: 0.92, F1-Score: 0.92\n",
    "Similar to macro average but considers the support (number of instances per class).\n",
    "\n",
    "#### Summary\n",
    "High Performance: The model performs very well overall, with an accuracy of 92%. Precision, recall, and F1-scores are generally high, indicating the model makes correct predictions and captures most of the relevant instances for each class.\n",
    "\n",
    "#### Class-specific Insights:\n",
    "Love and Anger: Outstanding performance, with high precision and recall.\n",
    "Fear and Surprise: Lower performance, particularly in recall for surprise, suggesting the model could improve in identifying these emotions.\n",
    "Balanced Metrics: The macro and weighted averages confirm that the model's performance is consistently good across different classes.\n",
    "\n",
    "#### Next Steps\n",
    "Improve Fear and Surprise Prediction:\n",
    "Consider collecting more data for these classes.\n",
    "Experiment with data augmentation techniques to enhance the model's ability to recognize these emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this block when you want to import the data from previously saved datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "working_base = pd.read_csv('datasets/twitter_sentiment_base_ready_reduced.csv')\n",
    "sentiments_features = working_base.loc[:, working_base.columns.str.startswith('feature_')].to_numpy()\n",
    "labels = working_base['label'].to_numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
